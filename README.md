# ü©∫ Assistant RAG Local (Diab√®te)

Ce projet est un **Assistant Conversationnel M√©dical** fonctionnant 100% en local sur votre machine.
Il utilise la technique **RAG (Retrieval-Augmented Generation)** pour r√©pondre aux questions en se basant exclusivement sur des documents fiables (ex: Ameli, F√©d√©ration des Diab√©tiques), garantissant la confidentialit√© des donn√©es.

**Stack Technique :** Python, Ollama, LangChain, ChromaDB, Streamlit, Ragas.

---

## üìã 1. Pr√©requis

Avant de commencer, assurez-vous d'avoir install√© :

1.  **Python 3.10+** : [T√©l√©charger Python](https://www.python.org/downloads/)
2.  **Ollama** : Le moteur d'IA local. [T√©l√©charger Ollama](https://ollama.com)

---

## ‚öôÔ∏è 2. Installation

### A. Cloner ou pr√©parer le dossier
Ouvrez votre terminal (Command Prompt, PowerShell ou Terminal) dans le dossier du projet.

### B. Cr√©er un environnement virtuel (Recommand√©)
Cela permet d'isoler les librairies du projet pour √©viter les conflits.

**Windows :**
```bash
python -m venv .venv
.venv\Scripts\activate
```

**Mac / Linux :**
```bash
python3 -m venv .venv
source .venv/bin/activate
```

# Installer les d√©pendances

Installez toutes les librairies Python n√©cessaires via pip :

```bash
pip install langchain langchain-community langchain-ollama langchain-chroma chromadb trafilatura streamlit ragas datasets pandas watchdog
```

# Configuration des Mod√®les(Ollama)

Le projet a besoin de 3 mod√®les sp√©cifiques pour fonctionner. Lancez ces commandes dans votre terminal une par une :

**Le Cerveau (Chat)** : Mod√®le l√©ger et rapide pour g√©n√©rer les r√©ponses.

```bash
ollama pull llama3.2
ollama pull nomic-embed-text
ollama pull mistral
```

# Utilisation

Suivez ces √©tapes dans l'ordre pour lancer le projet.

**√âtape 1 : R√©cup√©ration des Donn√©es (Scraping)**
Cette √©tape t√©l√©charge les articles depuis les URLs d√©finies dans ***config.py*** et cr√©e le fichier ***data/scraped_data.json***.

```bash
python main.py
```

**√âtape 2 : Lancer l'Assistant (Interface Web)**
C'est la m√©thode recommand√©e pour utiliser l'assistant. Cela lance une interface graphique dans votre navigateur.*

```bash
streamlit run app.py
```

L'application sera accessible √† l'adresse : http://localhost:8501

(Alternative : Pour tester en ligne de commande uniquement, lancez ***python main_local.py***)

# √âvaluation (Optionnel)

Pour tester la qualit√© scientifique des r√©ponses de votre IA (Fid√©lit√© et Pertinence) :

```bash
python eval_ragas.py
```

**Attention**: Ce script est gourmand en ressources. Il utilise le mod√®le **mistral** pour s'auto-√©valuer. Le processus est configur√© pour traiter les questions une par une afin d'√©viter de surcharger votre ordinateur. Les r√©sultats seront sauvegard√©s dans **resultats_evaluation.csv**.

# Structure du Projet

-> ***app.py*** : L'interface utilisateur (Frontend Streamlit).

-> ***rag_engine.py*** : Le c≈ìur du syst√®me. G√®re l'indexation ChromaDB et la g√©n√©ration de r√©ponses.

-> ***scraper.py*** : Script de r√©cup√©ration des donn√©es Web.

-> ***main.py*** : Point d'entr√©e pour lancer le scraping.

-> ***main_local.py*** : Interface de chat en ligne de commande (CLI).

-> ***eval_ragas.py*** : Script d'audit de qualit√© (utilise Ragas).

-> ***config.py*** : Configuration globale (URLs, chemins de fichiers).

-> ***db_storage_local/*** : Dossier cr√©√© automatiquement contenant la base de donn√©es vectorielle.


# D√©pannage

**Erreur "ChromaDB" ou modifications de donn√©es** : Si vous changez les donn√©es sources ou le mod√®le d'embedding, supprimez le dossier ***db_storage_local*** et relancez l'application. Elle reconstruira la base proprement.

**Lenteur** : C'est normal en local ("Inf√©rence CPU"). La vitesse d√©pend de la puissance de votre processeur/RAM.

**Une erreur "Dimension mismatch" ou ChromaDB crash** : Cela arrive si vous changez de mod√®le d'embedding. Supprimez simplement le dossier ***db_storage_local/*** et relancez ***streamlit run app.py***. Le dossier sera recr√©√© proprement.

**L'√©valuation Ragas est trop lente** : C'est normal en local. Le script est configur√© pour traiter 1 question √† la fois (***max_workers=1***) pour √©viter de faire planter votre ordinateur.

**L'IA r√©pond en anglais.** : Le prompt syst√®me dans ***rag_engine.py*** force le fran√ßais, mais les petits mod√®les (Llama 3.2) peuvent parfois d√©river. Relancez la question.