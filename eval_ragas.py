import os
import pandas as pd
from datasets import Dataset
from ragas import evaluate, RunConfig
from ragas.metrics import faithfulness, answer_relevancy
from langchain_ollama import ChatOllama, OllamaEmbeddings
from rag_engine import LocalRAGSystem

# ==========================================
# CONFIGURATION DU "JUGE" (IA LOCALE)
# ==========================================
print("‚öñÔ∏è  Configuration du Juge Ragas...")

JUDGE_MODEL_NAME = "mistral"

print(f"   Utilisation du mod√®le : {JUDGE_MODEL_NAME}")
judge_llm = ChatOllama(model=JUDGE_MODEL_NAME, temperature=0, format="json")
judge_embeddings = OllamaEmbeddings(model="nomic-embed-text")

# ==========================================
# DATASET DE TEST
# ==========================================
TEST_QUESTIONS = [
    "Quels sont les sympt√¥mes principaux du diab√®te ?",
    "Comment diagnostique-t-on un diab√®te de type 2 ?",
    # "Quels sont les chiffres du diab√®te en France ?",
    # "Quelle est la diff√©rence entre diab√®te type 1 et type 2 ?" 
]

# CORRECTION ICI : Ce sont des Strings simples, pas des listes ["..."]
GROUND_TRUTHS = [
    "Soif intense, urines abondantes, fatigue, perte de poids.",
    "Prise de sang √† jeun (glyc√©mie > 1,26 g/l √† deux reprises).",
    # "Plus de 3,5 millions de personnes trait√©es en 2020.",
    # "Le type 1 est auto-immun (insuline), le type 2 est li√© au mode de vie et √† l'√¢ge."
]

def build_dataset():
    """Pose les questions au RAG et construit le dataset"""
    rag = LocalRAGSystem()
    
    if not os.path.exists(rag.persist_directory):
        print("‚ö†Ô∏è Base de donn√©es introuvable. Lance d'abord 'main.py' puis 'main_local.py'.")
        return None

    print(f"ü§ñ Interrogation du RAG sur {len(TEST_QUESTIONS)} questions...")
    
    # CORRECTION ICI : Utilisation des noms de colonnes Ragas v0.2 officiels
    data = {
        "user_input": [],        # Au lieu de "question"
        "response": [],          # Au lieu de "answer"
        "retrieved_contexts": [],# Au lieu de "contexts"
        "reference": []          # Au lieu de "ground_truth"
    }

    for i, q in enumerate(TEST_QUESTIONS):
        print(f"   [{i+1}/{len(TEST_QUESTIONS)}] Question : {q}")
        try:
            result = rag.ask_with_context(q)
            
            data["user_input"].append(result["question"])
            data["response"].append(result["answer"])
            data["retrieved_contexts"].append(result["contexts"])
            data["reference"].append(GROUND_TRUTHS[i]) # Ajout de la string directe
        except Exception as e:
            print(f"‚ùå Erreur sur la question '{q}': {e}")

    return Dataset.from_dict(data)

def run_evaluation():
    dataset = build_dataset()
    if not dataset:
        return

    print("\nüìä Lancement de l'√©valuation (Patience, c'est lent en local)...")
    
    # On configure pour √©viter que √ßa plante si c'est trop long
    my_run_config = RunConfig(
        timeout=300,      # On laisse 5 minutes par question (large s√©curit√©)
        max_retries=1,    # On r√©essaie 1 fois en cas d'√©chec
        max_workers=1     # <--- LE SECRET : Une seule √©valuation √† la fois !
    )
    
    results = evaluate(
        dataset=dataset,
        metrics=[
            faithfulness,
            answer_relevancy
        ],
        llm=judge_llm,
        embeddings=judge_embeddings,
        run_config=my_run_config # <--- Ajout de la config
    )

    print("\nüèÜ R√âSULTATS :")
    print(results)

    # Sauvegarde
    df = results.to_pandas()
    df.to_csv("resultats_evaluation.csv", index=False)
    print("\nüíæ R√©sultats sauvegard√©s dans 'resultats_evaluation.csv'")

if __name__ == "__main__":
    run_evaluation()